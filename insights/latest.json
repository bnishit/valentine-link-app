{"updatedAt":"2026-02-08 12:40 UTC","tldr":["Vertical AI wins when trust + workflow integration are native, not bolted on.","Distribution rituals (weekly habits) compound faster than one-off launches.","Public narratives diverge from private fundamentalsâ€”watch unit economics, not headlines.","For ARPU, narrow premium modules outperform broad AI layers.","Best experiments include explicit decision rules before launch."],"episodes":[{"show":"Training Data (Sequoia)","title":"Vertical SaaS in an AGI World","notes":"- Adoption bottleneck is trust, not model quality.\n- Regulated workflows need audit trails + exception handling.\n- Buyers pay for reduced operational variance.\n- Framework: Reliability x Explainability x Workflow fit.\n- Experiment: paid AI workflow copilot for one regulated task. Metric: task completion time + error rate. Decision: keep if >25% faster and <= baseline error."},{"show":"20VC","title":"Navan public story and AI support","notes":"- Public market punishes narrative gaps quickly.\n- Building AI CS in-house made sense due to proprietary context.\n- Framework: Build vs Buy = Data leverage + integration depth + iteration speed.\n- Experiment: launch premium AI support lane for high-value segment. Metric: resolution time + NPS + upgrade rate. Decision: keep if NPS +8 and upsell +3pp."}],"rabbitHoles":["How to design auditable AI workflows for SMB finance ops","When does vertical AI justify separate pricing tiers?","Where does customer support AI create pricing power vs cost savings?","What leading indicators best predict ARPU lift for micro-modules?","How to operationalize decision rules in weekly experiment reviews?"]}